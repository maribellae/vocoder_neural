=========================================
SLURM_JOB_ID = 615303
SLURM_JOB_NODELIST = node5-21
SLURM_CPUS_ON_NODE = 4
SLURM_JOB_PARTITION = titan_X
SLURM_TASK_PID = 3308776
==========================================
/home/common/dorohin.sv/makarova/venv/bin/python3
The model has 32,584 trainable parameters
  0%|          | 0/2620 [00:00<?, ?it/s]  0%|          | 1/2620 [00:02<2:05:27,  2.87s/it]  0%|          | 2/2620 [00:03<1:16:36,  1.76s/it]  0%|          | 3/2620 [00:05<1:20:26,  1.84s/it]  0%|          | 4/2620 [00:06<1:03:41,  1.46s/it]  0%|          | 5/2620 [00:07<58:06,  1.33s/it]    0%|          | 6/2620 [00:10<1:23:28,  1.92s/it]  0%|          | 7/2620 [00:12<1:24:38,  1.94s/it]  0%|          | 8/2620 [00:14<1:17:14,  1.77s/it]  0%|          | 9/2620 [00:16<1:27:56,  2.02s/it]  0%|          | 9/2620 [00:19<1:33:39,  2.15s/it]
MyFastSpeech(
  (Embedding): Linear(in_features=1, out_features=256, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (Encoder): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (Decoder): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (1): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (2): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (3): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
      )
      (linear1): Linear(in_features=256, out_features=1024, bias=True)
      (linear2): Linear(in_features=1024, out_features=256, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (Duration): Duration(
    (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (linear): Linear(in_features=256, out_features=1, bias=True)
  )
  (Pitch): Duration(
    (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (linear): Linear(in_features=256, out_features=1, bias=True)
  )
  (Energy): Duration(
    (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (linear): Linear(in_features=256, out_features=1, bias=True)
  )
  (Projection): Linear(in_features=256, out_features=80, bias=True)
  (mylin3): Linear(in_features=10, out_features=40, bias=True)
  (myconv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))
  (mypool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (myconv2): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1))
  (myfc1): Linear(in_features=9792, out_features=120, bias=True)
  (myfc2): Linear(in_features=120, out_features=84, bias=True)
  (myfc3): Linear(in_features=84, out_features=10, bias=True)
)
ELEMENT  0
ELEMENT  1
ELEMENT  2
ELEMENT  3
ELEMENT  4
ELEMENT  5
ELEMENT  6
ELEMENT  7
ELEMENT  8
ELEMENT  9
Epoch   1 MEAN LOSS IS   11037.355322265625
